
# ğŸ“˜ USER GUIDE

## ğŸ”§ Project: LLM-Based Linux Driver Evaluation System

---

## ğŸ—‚ï¸ Overview

This project evaluates how well large language models (LLMs) like Mistral, Phi, CodeGemma perform at generating Linux character device drivers. The system automatically:

- Prompts LLMs with predefined Linux driver tasks
- Saves generated code
- Evaluates the output for correctness
- Summarizes performance using metrics

---

## ğŸ“ Folder Structure

```
linux_driver_eval/
â”œâ”€â”€ prompts/                 # Prompt text files for each model
â”œâ”€â”€ generated_code/          # Auto-generated C files from LLMs
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ metrics/             # Individual JSON reports per model
â”‚   â””â”€â”€ summary/             # Summary CSV comparing models
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/              # LLM interface and model registry
â”‚   â”œâ”€â”€ utils.py             # Code block extraction, saving
â”‚   â”œâ”€â”€ evaluator.py         # Static evaluation logic
â”‚   â”œâ”€â”€ summary.py           # CSV summary generation
â”‚   â”œâ”€â”€ logger.py            # Custom logging
â”‚   â””â”€â”€ compiler.py          # [Optional] Static code checks (if used)
â”œâ”€â”€ run_final_test.py        # âœ… Final script to run the full pipeline
â”œâ”€â”€ user_guide.md            # ğŸ“˜ Youâ€™re here!
â””â”€â”€ requirements.txt         # Python dependencies
```

---

## âš™ï¸ Prerequisites

- Python 3.8+
- An active [Together AI](https://www.together.ai/) account with API key
- Installed requirements:  
  ```bash
  pip install -r requirements.txt
  ```

---

## ğŸ” Environment Setup

1. Create a `.env` file in your project root:
    ```bash
    touch .env
    ```

2. Add your Together API key:
    ```
    TOGETHER_API_KEY=your_key_here
    ```

---

## ğŸš€ Running the Evaluation Pipeline

Use the main script:
```bash
python run_final_test.py
```

This will:
- Load each model and prompt
- Generate Linux driver code
- Evaluate code for quality and structure
- Save metrics in `reports/metrics/`
- Create a final summary in `reports/summary/summary.csv`

---

## ğŸ“Š Understanding the Results

Each modelâ€™s output is assessed on:

| Metric               | Description                                        |
|----------------------|----------------------------------------------------|
| `missing_functions`  | Key functions like `open`, `read`, `write`, etc.   |
| `missing_includes`   | Required kernel headers (`<linux/fs.h>`, etc.)     |
| `suspicious_macros`  | Usage of macros like `EFAULT`, `EIO`, etc.         |
| `function_score`     | Score out of 10 based on presence of core methods  |
| `optional_components`| Helpful extras like `file_operations`, `init` etc. |

View the final report at:  
`reports/summary/summary.csv`

---

## ğŸ§ª Custom Testing

You can manually test code generation, compilation, or evaluation using:
```bash
python src/test_compile.py
```

---

## ğŸ§° Troubleshooting

- **Missing API key**: Ensure `.env` has a valid `TOGETHER_API_KEY`
- **Clang errors**: You may ignore compilation if only static evaluation is required
- **WSL issues**: Make sure your virtual environment is set up inside WSL

---

## ğŸ“¦ To Submit

Include:
- This `user_guide.md`
- `reports/summary/summary.csv`
- Sample `.json` files in `reports/metrics/`
- A 3â€“5 minute screen recording demonstrating:
  - Code structure
  - Running the evaluation pipeline
  - Viewing the results
