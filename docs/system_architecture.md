# 📐 System Architecture

## Overview

This system evaluates Linux driver code generated by Large Language Models (LLMs). It consists of:
- Prompt runner + LLM interface
- Compilation + static analysis
- Code evaluator + rubric-based scoring

## Components

### 1. Prompt Runner (`run_pipeline.py`)
- CLI to choose prompt and model
- Loads prompt from `prompts/`
- Sends prompt to LLM (via Together API)
- Saves code to `generated_code/`

### 2. Compilation System (`compiler.py`)
- Uses `clang` with `-I mock_linux_headers` to bypass kernel headers
- Logs compiler output to `reports/compilation_logs/`
- Optional: `clang-tidy` static analysis

### 3. Evaluation System (`evaluator.py`)
- Scans for missing kernel driver components (functions, includes, macros)
- Saves JSON metrics report to `reports/metrics/`

### 4. Logger (`logger.py`)
- Unified terminal + file logs

## Flow Diagram

```
[prompt.txt] --> [LLM Model] --> [generated_code/file.c]
                                ↘
                         [compile + analyze]
                                ↘
                          [evaluate + score]
```

## Folder-Level View

```
/prompts → char_driver.txt
/generated_code → char_driver.c
/reports → metrics, logs
/src → compiler.py, evaluator.py, models/
```