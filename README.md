# Linux Driver Code Evaluation Framework ğŸš€

This project evaluates the quality of Linux device driver code generated by LLMs like Mistral, Starcoder, CodeGemma, and others. It compiles the generated C code, performs static analysis, checks for required kernel-level functions and includes, and produces detailed reports.

## ğŸ“ Directory Structure

```
â”œâ”€â”€ prompts/                  # Prompt files for LLMs
â”œâ”€â”€ generated_code/           # Output from LLMs (.c files)
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ compilation_logs/     # Logs from clang compiler
â”‚   â”œâ”€â”€ tidy_logs/            # Static analysis logs
â”‚   â””â”€â”€ metrics/              # Evaluation result JSONs
â”œâ”€â”€ mock_linux_headers/       # Dummy headers to bypass kernel deps
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ compiler.py           # Compilation + static analysis
â”‚   â”œâ”€â”€ evaluator.py          # Scoring based on rules
â”‚   â”œâ”€â”€ logger.py             # File + terminal logging
â”‚   â””â”€â”€ models/               # LLM model interfaces
â”œâ”€â”€ run_pipeline.py           # Main CLI entrypoint
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## âœ… Features

- Run Linux-driver prompts through LLMs via Together API
- Compile with `gcc` using mock kernel headers
- Optional static analysis 
- Custom evaluation via regex + rubric scoring
- Generates metrics reports in JSON
- Supports multiple model evaluations

## ğŸ”§ Setup

```bash
git clone https://github.com/yourname/linux-driver-evaluator.git
cd linux-driver-evaluator
python3 -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
```

## ğŸš€ Usage

```bash
python run_pipeline.py --model mistral --prompt char_driver.txt
```

## ğŸ“Š Evaluation Output

The final evaluation report is stored as:
```
reports/metrics/char_driver_mistral.json
```

## ğŸ“š Documentation

- [`user_guide.md`](user_guide.md) - How to run and evaluate
- [`system_architecture.md`](system_architecture.md) - Design and flow
- [`rubric.md`](rubric.md) - How evaluation metrics are scored

## ğŸ§  Models Supported

- qwen3_32b
- deepseek_r1
- afm_preview
- llama_vision
- deepseek_distill
- exaone_deep
- exaone_3_5
- llama3_turbo

## ğŸ‘¤ Author

Dhruv Thakur â€” [GitHub](https://github.com/dhruvthakur2000)